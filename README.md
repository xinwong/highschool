# 能让你“隐身”的对抗补丁

## 对抗补丁攻击现状
随着深度学习的迅速发展，深度神经网络已然成为当前计算机领域研究和应用最广泛的技术之一，成功应用于计算机视觉、自然语言处理、语音识别等多个领域。尽管深度神经网络已广泛应用于各种现实场景，但研究表明，其容易受到对抗样本的攻击，导致模型产生错误的输出，进而影响到实际应用系统的可靠性和安全性。目前大多数对抗攻击的研究主要集中于数字场景下的对抗样本，例如FGSM、BIM、PGD和C&W等方法。而在物理场景下的人工智能系统往往通过摄像头和传感器等设备获取输入，无法直接接受数字输入，因此在数字图像空间对输入样本进行细粒度逐维度修改的攻击方式就变得不切实际了。现实场景中的人工智能系统往往通过摄像头和传感器等设备获取输入，而无法直接接受数字输入。比如，在人脸识别场景中，用户的面部图像是通过摄像头采集的，而不是可以直接上传一张图片。在这些场景下，对输入样本进行细粒度逐维度修改的攻击方式就变得不切实际了，需要特殊的物理世界攻击(physical-world attack)方法来增强它们在真实环境中的对抗性。

## 方法
与基于L1、L2或L∞范数的全局扰动和难以察觉的数字场景下的对抗攻击不同，物理对抗攻击允许攻击者扭曲一个有界区域，牺牲了其隐匿性。物理场景下的对抗攻击最早被提出用于攻击图像分类，随后许多工作被提出来用来攻击目标检测、语义分割和图像检索等任务。相较于数字场景下的对抗攻击，物理场景下的对抗攻击具有其独特的挑战：（1）物理对抗样本需要考虑摄像头和传感器等设备成像的影响；（2）物理对抗样本需要考虑空间相对位置变化和环境变化的影响；（3）物理对抗样本还需考虑其隐匿性的问题。

（1）

<video src="Adversarial Patch.mp4" controls="controls" width="500" height="300"></video>




 
